\chapter{背景知识介绍}
\label{chap:back}

在本章中，我们介绍本文所涉及的背景知识。
在距离度量部分，介绍在机器学习和数据挖掘领域衡量数据节点之间距离的各种方法，
利用这些距离可以对节点进行分类、聚类等操作；
在聚类算法部分，介绍几种常见的聚类算法，通过对节点进行聚类，
可以挖掘出节点之间的关系。

\section{距离度量}
\label{sec:distance-metric}

在数据挖掘或机器学习的过程中，我们经常需要知道数据样本之间的差异，
用于评价样本之间的相似度或距离。知道样本之间的距离是很重要的，
比如著名的K最近邻分类算法（k-nearest neighbor algorithm, k-NN）就是利用节点之间的距离进行分类。
衡量节点之间的距离的方法有很多，本节介绍几种常用的衡量节点之间距离的方法。

距离度量可以看成是一个函数：

$$
d : X \times X \rightarrow R
$$

它把一个维度相同的向量对映射到一个实数集上，对于任意的向量$x, y, z$，
距离度量必须满足下面几个性质：

\begin{enumerate}
    \item 非负性：$ d(x, y) \geq 0 $；
    \item 不可区分者同一性： $ d(x, y) = 0$当且仅当$ x = y$；
    \item 对称性：$ d(x, y) = d(y, x)$；
    \item 三角不等性： $ d(x, z) = d(x, y) + d(y, z) $。
\end{enumerate}

\subsection{欧几里得距离}

欧几里得距离（Euclidean Distance）是最常见的两点之间的距离计算方法，简称为欧氏距离,
在多维空间中，节点$X = (x_1, x_2, ..., x_n) \in R ^ n$与节点$Y = (y_1, y_2, ..., y_n) \in R ^ n$之间的欧氏距离定义为：

\begin{equation}
    dist(X, Y) = \sqrt{ \sum_{i=1}^{n} (x_i - y_i)^2}
\end{equation}

欧氏距离也可以用向量形式表示：

\begin{equation}
    dist(X, Y) = \sqrt{(X - Y)^\top(X - Y)}
\end{equation}

\subsection{曼哈顿距离}

曼哈顿距离（Manhattan Distance）的正式定义为$L_1$-距离城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。
例如在平面上，坐标$(x_1, y_1)$的点P1与坐标$(x_2, y_2)$的点P2的曼哈顿距离为：

$$
    |x_1 - x_2| + |y_1 - y_2|
$$

在多维空间中，节点$X = (x_1, x_2, ..., x_n) \in R ^ n$与节点$Y = (y_1, y_2, ..., y_n) \in R ^ n$之间的曼哈顿距离定义为：

\begin{equation}
    dist(X, Y) = \sum_{i=1}^{n} |x_i - y_i|
\end{equation}

\subsection{切比雪夫距离}

切比雪夫距离（Chebyshev distance）或是$L_\infty$度量是向量空间中的一种度量，
二个点之间的距离定义为其各座标数值差的最大值。

在多维空间中，节点$X = (x_1, x_2, ..., x_n) \in R ^ n$与节点$Y = (y_1, y_2, ..., y_n) \in R ^ n$之间的切比雪夫距离距离定义为：

\begin{equation}
    dist(X, Y) = max(|x_i - y_i|), i = 1, 2, ..., n
\end{equation}

或利用极限来定义：

\begin{equation}
    dist(X, Y) = \lim_{p \rightarrow \infty}(\sum_{i=1}^{n} |x_i - y_i|^p)^{1/p}
\end{equation}

\subsection{明可夫斯基距离}

明可夫斯基距离（Minkowski Distance）是欧式距离的一中推广，
它定义了一种通用的距离计算方法，是对多个距离度量公式的概括性表述。
在多维空间中，节点$X = (x_1, x_2, ..., x_n) \in R ^ n$与节点$Y = (y_1, y_2, ..., y_n) \in R ^ n$之间的明可夫斯基距离定义为：

\begin{equation}
    dist(X, Y) = (\sum_{i=1}^{n} |x_i - y_i|^p)^{1/p}
\end{equation}

其中$p$是一个可变的参数，根据不同的$p$,有不同的距离计算方法：

\begin{itemize}
    \item 当$p = 1$，此时的距离就是曼哈顿距离；
    \item 当$p = 2$，此时的距离就是欧氏距离；
    \item 当$p \rightarrow \infty$，此时的距离就是切比雪夫距离。
\end{itemize}

\subsection{马氏距离}

在欧式距离中，数据的各个维度对于整个计算数据点之间距离所起到的作用是均等的，
而有时，有些维度可能相对于其他的维度具有更高的重要性，应该被赋予更高的权重。
考虑到这种关系，印度统计学家马哈拉诺比斯（P. C. Mahalanobis）提出了马氏距离（Mahalanobis Distance）。
在多维空间中，节点$X = (x_1, x_2, ..., x_n) \in R ^ n$与节点$Y = (y_1, y_2, ..., y_n) \in R ^ n$之间的马氏距离定义为：

\begin{equation}
    dist(X, Y) = \sqrt{(X - Y)^\top M (X - Y)}
\end{equation}

其中$M \in R ^{n * n}$称为调整矩阵，挑选出合适的$M$是不容易的，
大多数需要采用机器学习的方式来获取$M$，使得计算出的距离能够更贴近于数据的真实情况，
让已知的相似的节点距离更近。通过机器学习获取$M$的过程成为距离度量学习（distance metric learning），
在第\ref{chap:dca}章中，我们详细阐述了本文所提出的距离度量学习算法。

\section{聚类算法}
\label{sec:cluster}

聚类分析（Cluster analysis）是一种数据挖掘技术,
它在很多领域得到了广泛的应用，包括机器学习，数据挖掘，模式识别，图像分析以及生物信息技术。
聚类分析是把数据节点分到一个个组，让相似或相近的节点处在同一个组中。
聚类分析是一种非监督的机器学习方法（Unsupervised Learning），因为聚类时不需要利用节点的标签信息。
在本节中，我们介绍几种常见的聚类算法。

\subsection{kmeans算法}

kmeans算法是一种使用最广泛的聚类算法，给定聚类的个数$k$，
kmeans算法能够把数据划分为$k$个聚类。

基本的kmeans算法如算法\ref{algo:kmeans}所示，
它的目标是找到$k$个中心节点表示$k$个聚类，
有了这$k$个中心节点之后，每一个节点就被划分到离它最近的那个中心节点所表示的聚类中。
聚类过程如下：
首先，适当地选取$k$个节点作为中心节点（随机选取$k$个节点是一种最简单的方法），然后不停地迭代来调整中心节点，
如果中心节点不再变化，那么迭代终止。
在每一轮迭代中，利用$k$个中心节点得到$k$个聚类，
然后重新计算每一个聚类的中心节点, 聚类$C_i$的中心节点计算方法为：

\begin{equation}
    c_i = \frac{1}{m_i} \sum_{X \in C_i} X
\end{equation}

其中$m_i$是聚类$C_i$所包含的节点个数。

\begin{algorithm}[htb]
    \caption{基本的kmeans算法}
    \label{algo:kmeans}
    \begin{algorithmic}[1]
        \State 选定$k$个节点作为初始节点；
        \Repeat
            \State 通过把节点添加到离它最近的中心节点所表示的聚类形成$k$个聚类；
            \State 重新计算每一个聚类的中心节点；
        \Until{中心节点不再变化}
    \end{algorithmic}
\end{algorithm}

为了保证K-Means算法的具有比较好的聚类效果，必须提供合适的$k$值，
对于一般的聚类问题来说，这是比较难做到的。因此，必须尝试各种不同的$k$值，
选择其中的最好的结果。在本文中，我们使用kmeans算法与本文提出的算法做对比，
用于验证本文算法的有效性。

\subsection{凝聚层次聚类算法}

凝聚层次聚类算法（Agglomerative Hierarchical Clustering）是分层聚类（Hierarchical Clustering）算法的一种\upcite{arenas2008analysis,ronhovde2009multiresolution,lancichinetti2009detecting,sales2007extracting}，
它采用自底向上的策略，首先将每一个对象作为一个类，
然后根据某种度量(如2个当前类中心点的距离)将这些类合并为较大的类，
不停地重复合并的过程，
直到所有的对象都在一个类中，或者是满足某个终止条件时为止。
和kmeans算法一样，凝聚层次聚类算法也是一种比较经典的聚类算法，
虽然它已经被提出了很多年，但仍然具有广泛的应用，
本文提到的DSHRINK算法就是借鉴了凝聚层次聚类算法的思想设计而成。
基本层次聚类算法的形式化描述见算法\ref{algo:agg-hierarch-cluster}。

\begin{algorithm}[htb]
    \caption{基本的凝聚层次聚类算法}
    \label{algo:agg-hierarch-cluster}
    \begin{algorithmic}[1]
        \State 初始化每一个节点为一个单独的聚类；
        \Repeat
            \State 把两个距离最近的聚类组合成一个新的聚类；
            \State 重新计算这个新的聚类与其他聚类之间的距离；
        \Until{只剩一个聚类或某个特定的终止条件}
    \end{algorithmic}
\end{algorithm}

在算法\ref{algo:agg-hierarch-cluster}的第4行，
需要重新计算这个新的聚类与其他聚类之间的距离。
此时可以采用三种不同的的计算距离的方法：

\begin{enumerate}
    \item 最小值: 如果合并聚类A和聚类B成聚类C，此时C与其他任何一个聚类D的距离为：
        $dist(C, D) = min\{dist(A, D), dist(B, D)\}$
    \item 最大值: 如果合并聚类A和聚类B成聚类C，此时C与其他任何一个聚类D的距离为：
        $dist(C, D) = max\{dist(A, D), dist(B, D)\}$
    \item 平均值: 如果合并聚类A和聚类B成聚类C，此时C与其他任何一个聚类D的距离为：
        $dist(C, D) = \frac{1}{2}(dist(A, D) + dist(B, D))$
\end{enumerate}

在实际的聚类过程中，可以根据不同的需要选择不同的计算方法。

\subsection{谱聚类算法}

谱聚类算法建立在谱图理论基础上，与kmeans算法相比，
它具有能在任意形状的样本空间上聚类且收敛于全局最优解的优点\upcite{ng2002spectral}。
谱聚类算法的本质是将聚类问题转化为图的最优划分问题，是一种点对聚类算法。

谱聚类算法将数据集中的每个对象看作是图的顶点$V$，将顶点间的相似度量化作为相应顶点连接边$E$的权值，
这样就得到一个基于相似度的无向加权图$G(V, E)$，于是聚类问题就可以转化为图的划分问题。
基于图论的最优划分准则就是使划分成的子图内部相似度最大，子图之间的相似度最小。

基于不同的准则函数及谱映射方法，谱聚类算法有着不同的具体实现方法，但是这些实现方法都可以归纳为下面三个主要步骤：

\begin{enumerate}
    \item 构建表示对象集的相似度矩阵；
    \item 通过计算相似度矩阵或拉普拉斯矩阵的前k个特征值与特征向量，构建特征向量空间；
    \item 利用kmeans或其它经典聚类算法对特征向量空间中的特征向量进行聚类。
\end{enumerate}

在本文中，我们实现了一个谱聚类算法来验证机器度量学习的正确性。
它实现起来非常简单，只需要用到矩阵求特征值和特征向量的操作，
它的matlab代码如下：

\begin{lstlisting}[language={matlab}, caption={谱聚类算法的简单实现}]
% 将距离矩阵转换为相似度矩阵
n=size(distance,1);
tmp=sum(sum(distance))/n/(n-1);
W = exp(-distance.^2/2/tmp/tmp);

% 构建特征向量空间
D = diag(sum(W));
L = D-W;
k = 6;
opt = struct('issym', true, 'isreal', true);
[V dummy] = eigs(L, D, k+1, 'SM', opt);

% 基于特征向量进行聚类
spectural_idx = kmeans(V(:,1:k), k);

\end{lstlisting}


\section{本章小节}

在本章中，我们详细阐述了本文需要用到的关于距离和聚类的背景知识。
本章中的内容大部分来自于《数据挖掘导论》这本数据挖掘的入门书籍\upcite{tan2007introduction}。
了解这些知识对于理解本文具有很大的帮助。本文第\ref{chap:dca}章中的距离学习算法
和第\ref{chap:dshrink}章聚类算法分别使用了本章提到的距离和聚类的知识作为理论基础。
