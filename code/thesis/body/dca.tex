%%==================================================
%% chapter04.tex: DCA
%% Encoding: UTF-8
%%==================================================

\chapter{距离度量学习}
\label{chap:dca}

在这个部分中，我们解决如何利用已知的局部信息网络的信息去学习一个距离度量（distance metric），
并通过这个度量计算任意节点对之间的距离。

欧式距离（Euclidean distance）是一种简单的计算节点之间距离的方法，
它仅仅利用两个节点的属性向量去计算两个节点之间的距离。
有时，节点之间的距离不仅仅和节点的属性有关，
因此需要一种方法在欧式距离的基础上进行一定的调整，
让已知的相似的节点之间的距离更近一些，而让不相似的节点之间的距离更远一些。
在这种情况下，我们采用马氏距离（Mahalanobis distance）来表示节点之间的距离，
而计算马式距离的最主要的任务是要求得马氏转换矩阵（Mahalanobis matrix）$M$。

\section{问题的数学定义}
\label{sec:dcadef}

\begin{defn}{正语境限制}
\label{defn:posconstraints}

    正语境限制是指已知某些节点之间是相似的。
    正语境限制通过一个相似集列表$C$给出，
    $C$中的每一个元素$C_i$都代表一个由相似的节点组成的集合。
    即：

    $$
    x_i\text{与}x_j\text{相似} \Longleftrightarrow \exists k \ x_i \in C_k \text{且} x_j \in C_k 
    $$

\end{defn}

\begin{defn}{负语境限制}
\label{defn:negconstraints}

    负语境限制限定了正语境中规定的相似集之间是否是不相似的。
    负语境限制通过一个与$C$同样长度的列表$D$给出，
    对于每一个$C_i$，$D_i$表示$C$中所有与$C_i$不相似的相似集组成的结合。
    即：

    $$
    D_i = \{C_j | C_i \text{与} C_j \text{不相似} \}
    $$

\end{defn}


已知数据集
$X = \{ \bm{x}_i \}_{i=1}^{N}$
以及在数据集上的正语境限制$C$和负语境限制$D$，
$C$和$D$的长度都为$n$，
判别成分分析（Discriminative Component Analysis，DCA）算法
通过学习一个转换矩阵$M$，使得通过这个矩阵计算出的节点距离让相似的节点之间的距离更近，
不相似的的节点之间的距离更远。

为了完成判别成分分析，定义两个协方差矩阵$\hat{C_b}$和$\hat{C_w}$分别表示
相似集之间的总方差和相似集内部的总方差。$\hat{C_b}$和$\hat{C_w}$的定义如下：

\begin{align}
    \hat{C_b} &= \frac{1}{n_b} \sum_{j=1}^n \sum_{i \in D_j} (\bm{m}_j - \bm{m}_i)(\bm{m}_j - \bm{m}_i)^\top \\
    \hat{C_w} &= \frac{1}{n_b} \sum_{j=1}^n \frac{1}{n_j} \sum_{i = 1}^{n_j} (\bm{x}_{ij} - \bm{m}_j)(\bm{x}_{ij} - \bm{m}_j)^\top
\end{align}

其中$n_b = \sum_{j=1}^{n}|D_j|$, $|\bullet|$表示集合的基数，
$n_j = |C_j|$,
$\bm{m}_j$表示相似集$C_j$的平均向量，亦即
$\bm{m}_j = \frac{1}{n_j} \sum_{i=1}^{n_j} \bm{x}_{ji}$, 
$\bm{x}_{ji}$是指$C_j$中的第$i$个元素。

DCA算法的目的是找到一个对于距离度量的最优线性转换
使得相似集之间的总方差最大而相似集内部的总方差最小，
也就是求解下面的最优化问题：

$$
\begin{equation}
    \label{equa:dca_opt}
    J(A) = \operatorname*{arg\,max}_A \frac {|A^\top \hat{C_b} A|} {|A^\top \hat{C_w} A|}
\end{equation}
$$

当找到最优的$A$, 最优的马氏转换矩阵也就是$M = A^\top A$。

\section{DCA算法的求解}
\label{sec:algorithm_dca}

根据Fisher定理\upcite{mika1999fisher},
等式\ref{equa:dca_opt}的最优解是一个同时使$\hat{C_b}$和$\hat{C_w}$对角化的转换矩阵。
求解的详细过程如下：

\begin{itemize}
    \item 使用特征分析把$\hat{C_b}$对角化： 
        \begin{itemize}
            \item 找到一个矩阵$U$满足$U^\top\hat{C_b}U = \Lambda_b, U^\topU = I$，
                其中$\Lambda_b$是一个升序的对角矩阵;
            \item 选取$U$的后面$k$个特征值不为$0$的特征向量组成矩阵$\hat{U}$;
            \item 求得$D_b = \hat{U}^\top\hat{C_b}\hat{U}$;
            \item 求得$Z = \hat{U}D_b^{-1/2}, C_z = Z^\top\hat{C_w}Z$;
        \end{itemize}

    \item 使用特征分析把$C_z$对角化： 

        找到一个矩阵$V$满足$V^\top C_z V = \Lambda_w, V^\top V = I$，
                其中$\Lambda_w$是一个升序的对角矩阵。

    \item 最终$A = Z\hat{V}\Lambda_w^{-1/2}, M = A^\top A$。 
        
\end{itemize}
