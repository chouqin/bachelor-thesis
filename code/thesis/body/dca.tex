\chapter{距离度量学习}
\label{chap:dca}


机器学习算法在很多的领域得到了广泛的应用，
机器学习的很多经典算法，比如说kmeans聚类算法和k近邻分类算法，
需要有一定的方式定义节点之间的距离，有了这个距离之后算法才能得以运用。
在很多情况下，我们可以简单地使用欧氏距离作为节点之间的距离，
但有时，仅仅使用欧氏距离用于机器学习算法可能会导致一个不好的结果，
因为距离度量的好坏能够很大程度地影响机器学习算法。
因此，学习一个合适的距离度量对于机器学习是十分重要的。

欧式距离是一种简单的计算节点之间距离的方法，
它仅仅利用两个节点的属性向量去计算两个节点之间的距离。
有时，节点之间的距离不仅仅和节点的属性有关，
因此需要一种方法在欧式距离的基础上进行一定的调整，
让已知的相似的节点之间的距离更近一些，而让不相似的节点之间的距离更远一些。
在这种情况下，我们采用马氏距离来表示节点之间的距离，
而计算马氏距离的最主要的任务是要求得马氏转换矩阵（Mahalanobis matrix）$M$。
马氏距离的数学定义为：

\begin{equation}
    d_M(X, Y) = \sqrt{(X - Y)^\top M (X - Y)}
\end{equation}

其中，$M$必须是一个正定矩阵来满足距离的非负性和三角不等式性质。
同时，我们又可以把$M$进行分解，把$M$表示为$M = A^\top A$，其中$A$成为数据转换矩阵。
距离度量学习的目标就是找到一个最优的马氏转换矩阵$M$或者是最优的数据转换矩阵$A$。

在本章中，我们解决如何利用已知的局部信息网络的信息去学习一个距离度量，
并通过这个度量计算任意节点对之间的距离。
尽管在社交网络中，我们无法获取整个社交网络的全部信息，
但我们可以通过一定的技术手段获取到几个局部信息网络的信息，
其中每个局部信息网络中的任意节点的信息都是完整的。
利用这些局部网络的信息，我们可以把它转换为在定义\ref{defn:posconstraints}定义的正语境限制
和在定义\ref{defn:negconstraints}定义的负语境限制，
然后，使用本章提出的DCA算法，可以在结合正语境限制和负语境限制学习到一个能够正确反映节点之间的结构相似度的距离度量。

首先，我们给出几个数学定义。

\section{问题的数学定义}
\label{sec:dcadef}

\begin{defn}{正语境限制}
\label{defn:posconstraints}

    正语境限制是指已知某些节点之间是相似的。
    正语境限制通过一个相似集列表$C$给出，
    也就是说$C$是一个列表，
    $C$中的每一个元素$C_i$都代表一个由相似的节点组成的集合。
    即：

    $$
    x_i\text{与}x_j\text{相似} \Longleftrightarrow \exists k \ x_i \in C_k \text{且} x_j \in C_k 
    $$

\end{defn}

正语境限制限制了某些节点之间是相似的，也就是说，
它限制了某些节点之间的距离应该较小。相似节点之间的距离越小，越能满足正语境限制。

\begin{defn}{负语境限制}
\label{defn:negconstraints}

    负语境限制限定了正语境中规定的相似集之间是否是不相似的。
    负语境限制通过一个与$C$同样长度的列表$D$给出，
    对于每一个$C_i$，$D_i$表示$C$中所有与$C_i$不相似的相似集组成的集合。
    即：

    $$
    D_i = \{C_j | C_i \text{与} C_j \text{不相似} \}
    $$

\end{defn}

负语境限制限制了某些节点之间是不相似的，也就是说，
它限制了某些节点之间的距离应该较大。不相似节点之间的距离越大，越能满足正语境限制。

利用局部信息网络的信息，我们可以得到正语境限制和负语境限制。
已知数据集
$X = \{ \bm{x}_i \}_{i=1}^{N}$
以及在数据集上的正语境限制$C$和负语境限制$D$，
$C$和$D$的长度都为$n$，
判别成分分析（Discriminative Component Analysis，DCA）算法
通过学习一个转换矩阵$M$，使得通过这个矩阵计算出的节点距离让相似的节点之间的距离更近，
不相似的的节点之间的距离更远。

为了完成判别成分分析，我们先定义两个协方差矩阵$\hat{C_b}$和$\hat{C_w}$分别表示
相似集之间的总方差和相似集内部的总方差。$\hat{C_b}$和$\hat{C_w}$的定义如下：

\begin{align}
    \label{equa:cb}
    \hat{C_b} &= \frac{1}{n_b} \sum_{j=1}^n \sum_{i \in D_j} (\bm{m}_j - \bm{m}_i)(\bm{m}_j - \bm{m}_i)^\top \\
    \label{equa:cw}
    \hat{C_w} &= \frac{1}{n} \sum_{j=1}^n \frac{1}{n_j} \sum_{i = 1}^{n_j} (\bm{x}_{ij} - \bm{m}_j)(\bm{x}_{ij} - \bm{m}_j)^\top
\end{align}

其中$n$是$C$或者$D$的长度，$n_b = \sum_{j=1}^{n}|D_j|$, $|\bullet|$表示集合的基数，
$n_j = |C_j|$,
$\bm{m}_j$表示相似集$C_j$的平均向量，亦即
$\bm{m}_j = \frac{1}{n_j} \sum_{i=1}^{n_j} \bm{x}_{ji}$, 
其中$\bm{x}_{ji}$是指$C_j$中的第$i$个元素。

DCA算法的目的是找到一个对于距离度量的最优线性转换矩阵$A$
使得相似集之间的总方差最大而相似集内部的总方差最小，
也就是求解下面的最优化问题：

\begin{equation}
    \label{equa:dca_opt}
    J(A) = \operatorname*{arg\,max}_A \frac {|A^\top \hat{C_b} A|} {|A^\top \hat{C_w} A|}
\end{equation}

当找到最优的$A$, 最优的马氏转换矩阵也就是$M = A^\top A$。

\section{使用DCA算法进行求解}
\label{sec:algorithm_dca}

根据Fisher定理\upcite{mika1999fisher, mclachlan2004discriminant},
等式\ref{equa:dca_opt}的最优解是一个同时使$\hat{C_b}$和$\hat{C_w}$对角化的转换矩阵。
为了有效地求得最优解，我们提出了DCA算法。
DCA算法的详细过程见算法\ref{algo:dca}，使用DCA算法，
首先使用特征分析把相似集之间的协方差矩阵$\hat{C_b}$对角化,
找到一个矩阵$U$满足$U^\top \hat{C_b} U = \Lambda_b, U^\top U = I$，
其中$\Lambda_b$是一个升序的对角矩阵。
通过去掉$U$中特征向量为0的特征向量，我们能够得到一个
$k \times k$的矩阵$D_b$，这样我们能够得到一个转化矩阵$Z = \hat{U}D_b^{-1/2}$。
利用它我们可以得到一个矩阵$C_z = Z^\top\hat{C_w}Z$和矩阵$V$满足
满足$V^\top C_z V = \Lambda_w, V^\top V = I$，
最终$A = Z\hat{V}\Lambda_w^{-1/2}, M = A^\top A$。 
 
%\begin{itemize}
    %\item 首先使用特征分析把$\hat{C_b}$对角化： 
        %\begin{itemize}
            %\item 找到一个矩阵$U$满足$U^\top \hat{C_b} U = \Lambda_b, U^\top U = I$，
                %其中$\Lambda_b$是一个升序的对角矩阵;
            %\item 选取$U$的后面$k$个特征值不为$0$的特征向量组成矩阵$\hat{U}$;
            %\item 求得$D_b = \hat{U}^\top\hat{C_b}\hat{U}$;
            %\item 求得$Z = \hat{U}D_b^{-1/2}, C_z = Z^\top\hat{C_w}Z$;
        %\end{itemize}

    %\item 使用特征分析把$C_z$对角化： 

        %找到一个矩阵$V$满足$V^\top C_z V = \Lambda_w, V^\top V = I$，
                %其中$\Lambda_w$是一个升序的对角矩阵。

    %\item 最终$A = Z\hat{V}\Lambda_w^{-1/2}, M = A^\top A$。 
        
%\end{itemize}

\begin{algorithm}[htb]
    \caption{DCA算法}
    \label{algo:dca}
    \begin{algorithmic}[1]
        \Require
        数据集$X = \{ \bm{x}_i \}_{i=1}^{N}$,
        长度为n正语境限制$C$和负语境限制$D$        
        \Ensure
        最优的马氏转换矩阵$M$
        \State 根据等式\ref{equa:cb}和\ref{equa:cw}计算$\hat{C_b}$和$\hat{C_w}$;
        \State 对角化$\hat{C_b}$： 
        \State 求得矩阵$U$使得$U^\top \hat{C_b} U = \Lambda_b, U^\top U = I$，
            其中$\Lambda_b$是一个升序的对角矩阵;
        \State $\hat{U}$ = $U$的后面$k$个特征值不为$0$的特征向量;
        \State $D_b = \hat{U}^\top\hat{C_b}\hat{U}$;
        \State $Z = \hat{U}D_b^{-1/2}, C_z = Z^\top\hat{C_w}Z$;
        \State 求得矩阵$V$使得$V^\top C_z V = \Lambda_w, V^\top V = I$，
                其中$\Lambda_w$是一个升序的对角矩阵;
        \State $A = Z\hat{V}\Lambda_w^{-1/2}$; 
        \State $M = A^\top A$; 
        \Return $M$;
    \end{algorithmic}
\end{algorithm}

可以看出，DCA算法实现起来十分简单，可以通过基本的矩阵运算来进行实现，
相对于其他的一些距离度量学习算法\upcite{xing2002distance,Lin:2012:CDI:2187836.2187883}，
DCA算法不需要求解凸型最优化问题，可以极大地减少运算量，具有很高的效率。
同时DCA算法相对于RCA算法来说\upcite{bar2006learning}，
考虑到了负语境限制，因此能够保证学习到的距离度量更加符合数据本身的特点。

%DCA算法具体实现见\ref{code:dca}。

\section{本章小节}

在本章中，我们提出了一种距离度量学习的算法——DCA，
它能够利用信息网络的节点属性$X$，正语境限制$C$和负语境限制$D$，
学习到一个距离度量$M$，
通过$M$计算出的节点对之间的距离能够保证相似的节点之间拥有更近的距离。
